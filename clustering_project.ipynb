{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1edad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "# Visualizing\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "#Visual format\n",
    "pd.options.display.float_format = '{:20,.4f}'.format\n",
    "\n",
    "#my libraries\n",
    "from wrangle import get_zillow_data, wrangle_zillow, remove_outliers, train_validate_test_split, get_hist, get_box, summarize\n",
    "from explore import inertia, variable_distributions, plot_against_target\n",
    "import evaluate\n",
    "import model\n",
    "import env\n",
    "\n",
    "#library imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression, LassoLars\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Statistical Tests\n",
    "import scipy.stats as stats\n",
    "\n",
    "#alpha\n",
    "alpha = .05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d3b0f1",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "\n",
    "- Continuing with the Zillow 2017 properties and predictions for single unit family homes I am looking for what is driving the errors in the Zestimates.\n",
    "\n",
    "- Of the five tested features against, log error is dependent of age, sqft and home value.\n",
    "\n",
    "- Through exploration and statistical testing I found homes over 50, homes with less than 1000 sqft, and homes values less than 250,000 are the biggest drivers of errors.\n",
    "\n",
    "- In feature engineering my model did do better then the baseline; however, modeling on sqft my data with a baseline of 0.165 my test data came back with an RMSE of 0.175.\n",
    "\n",
    "- What I would like to test is exactly where the highest amount of the homes are located.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b88fb8f",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "- My plan is to take the 2017 properties and predictions through the data pipeline in order to find log error drivers and prepare a model to predict future errors.\n",
    "\n",
    "- Select the <a href=\"https://trello.com/invite/b/EMEzPn69/2f064c89555d288b2b4b55618ceaceab/clusteringproject\">link</a> to find a copy of my trello board I used in planning and executing this project.\n",
    "\n",
    "- Objectives:\n",
    "    - Is a higher log error dependent on homes over 50 years old? (Cluster - 2)\n",
    "    - Is a higher log error dependent on homes less 1000 sqft? (Cluster - 6)\n",
    "    - Is a higher log error dependent on homes who's ppsqft is less 200? (Cluster - 7)\n",
    "    - Is a higher log error dependent on homes with a smaller lot size? (Cluster - 8)\n",
    "    - Is a higher log error dependent on less expensive homes? (Cluster - 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff09d8a3",
   "metadata": {},
   "source": [
    "## Acquired\n",
    "\n",
    "- I acquired my data via the zillow SQL database, importing all tables (LEFT JOIN), sub query DISTINCT id from 2017 properties, WHERE statement for 2017% from the predictions table and where latitude iS NOT NULL.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6823a8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>typeconstructiontypeid</th>\n",
       "      <th>storytypeid</th>\n",
       "      <th>propertylandusetypeid</th>\n",
       "      <th>heatingorsystemtypeid</th>\n",
       "      <th>buildingclasstypeid</th>\n",
       "      <th>architecturalstyletypeid</th>\n",
       "      <th>airconditioningtypeid</th>\n",
       "      <th>id</th>\n",
       "      <th>basementsqft</th>\n",
       "      <th>...</th>\n",
       "      <th>id</th>\n",
       "      <th>logerror</th>\n",
       "      <th>transactiondate</th>\n",
       "      <th>airconditioningdesc</th>\n",
       "      <th>architecturalstyledesc</th>\n",
       "      <th>buildingclassdesc</th>\n",
       "      <th>heatingorsystemdesc</th>\n",
       "      <th>propertylandusedesc</th>\n",
       "      <th>storydesc</th>\n",
       "      <th>typeconstructiondesc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14297519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>261.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1727539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17052889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>261.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1387261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   parcelid  typeconstructiontypeid  storytypeid  propertylandusetypeid  \\\n",
       "0  14297519                     NaN          NaN               261.0000   \n",
       "1  17052889                     NaN          NaN               261.0000   \n",
       "\n",
       "   heatingorsystemtypeid  buildingclasstypeid  architecturalstyletypeid  \\\n",
       "0                    NaN                  NaN                       NaN   \n",
       "1                    NaN                  NaN                       NaN   \n",
       "\n",
       "   airconditioningtypeid       id  basementsqft  ...  id             logerror  \\\n",
       "0                    NaN  1727539           NaN  ...   0               0.0256   \n",
       "1                    NaN  1387261           NaN  ...   1               0.0556   \n",
       "\n",
       "   transactiondate  airconditioningdesc  architecturalstyledesc  \\\n",
       "0       2017-01-01                 None                    None   \n",
       "1       2017-01-01                 None                    None   \n",
       "\n",
       "   buildingclassdesc  heatingorsystemdesc        propertylandusedesc  \\\n",
       "0               None                 None  Single Family Residential   \n",
       "1               None                 None  Single Family Residential   \n",
       "\n",
       "   storydesc  typeconstructiondesc  \n",
       "0       None                  None  \n",
       "1       None                  None  \n",
       "\n",
       "[2 rows x 69 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_zillow_data()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c43604",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d0d8ed",
   "metadata": {},
   "source": [
    "##### Key Takeaways\n",
    "1. A total of 77579 rows and 69 columns.\n",
    "2. I will need to rename columns for an easier read.\n",
    "3. Find which columns/rows I will need to keep or drop.\n",
    "4. There is a large number of missing values and I will need to figure out how to deal with those in the prepare section.\n",
    "5. Convert data types as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049386a2",
   "metadata": {},
   "source": [
    "## Prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa6d8c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>sqft</th>\n",
       "      <th>county_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>lot_size</th>\n",
       "      <th>tax_value</th>\n",
       "      <th>logerror</th>\n",
       "      <th>county</th>\n",
       "      <th>age</th>\n",
       "      <th>tax_rate</th>\n",
       "      <th>price_per_sqft</th>\n",
       "      <th>abs_logerror</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3100</td>\n",
       "      <td>6059</td>\n",
       "      <td>33,634,931.0000</td>\n",
       "      <td>-117,869,207.0000</td>\n",
       "      <td>4506</td>\n",
       "      <td>1023282</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>Orange</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>330</td>\n",
       "      <td>0.0256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1465</td>\n",
       "      <td>6111</td>\n",
       "      <td>34,449,266.0000</td>\n",
       "      <td>-119,281,531.0000</td>\n",
       "      <td>12647</td>\n",
       "      <td>464000</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>Ventura</td>\n",
       "      <td>54</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>316</td>\n",
       "      <td>0.0556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bathrooms  bedrooms  sqft  county_code             latitude  \\\n",
       "0          3         4  3100         6059      33,634,931.0000   \n",
       "1          1         2  1465         6111      34,449,266.0000   \n",
       "\n",
       "             longitude  lot_size  tax_value             logerror   county  \\\n",
       "0    -117,869,207.0000      4506    1023282               0.0256   Orange   \n",
       "1    -119,281,531.0000     12647     464000               0.0556  Ventura   \n",
       "\n",
       "   age             tax_rate  price_per_sqft         abs_logerror  \n",
       "0   23               0.0108             330               0.0256  \n",
       "1   54               0.0122             316               0.0556  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = wrangle_zillow(df)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead946a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cc3ff3",
   "metadata": {},
   "source": [
    "##### Key Takeaways\n",
    "1. Reduced total number of columns to 71359 and number of rows to 14.\n",
    "2. Drop columns missing 70% of values.\n",
    "3. Drop columns missing 50% of values.\n",
    "4. Remain nulls were filled with the respective average.\n",
    "5. Filtered all, but single use properties 'propertylandusetypeid'. A list of id's are annotated in the wrangle.py.\n",
    "6. Filtered out homes having no bedrooms or baths.\n",
    "7. Add a 'county' column with the county name using 'fips' codes. Kept 'fips' later renamed 'county_code' for explortion and clustering.\n",
    "8. Dropped all unnecessary columns based on initial counts, visualizations and hypothesis. A list of dropped columns can be fo und in the wrangle.py.\n",
    "9. Caluclated and added columns for home 'age', 'tax_rate', and 'price_per_sqft'.\n",
    "10. Converted integers, left decimal numbers as floats and name of counties, 'county' as an object.\n",
    "11. In my initial exploration of the raw data I removed any outliers below the 25 percentile and above 75 percentile.\n",
    "12. The .describe below is a reflection of the clean data with all outliers remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d4b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de64dd6",
   "metadata": {},
   "source": [
    "### Split\n",
    "- Here I am splitting my data into train, validate, and test for further exploration and modeling once the split data is scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e83cf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train observations by shape:  (39960, 14)\n",
      "validate observations by shape:  (17127, 14)\n",
      "test observations by shape:  (14272, 14)\n"
     ]
    }
   ],
   "source": [
    "train, validate, test = train_validate_test_split(df)\n",
    "print(\"train observations by shape: \", train.shape)\n",
    "print(\"validate observations by shape: \", validate.shape)\n",
    "print(\"test observations by shape: \", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57272a63",
   "metadata": {},
   "source": [
    "## Explore\n",
    "\n",
    "- I get a look at the train df to help determine which variables I'd like to take a look at using a histogram.\n",
    "- I used .ticklabelformat to format the axis style. I used 'sci' as my style so the numbers use poower of tens foor readability.\n",
    "- I'll view those variables and view them agains the target 'logerror' using .regplot.\n",
    "- I used the same .tickplot_format as above for readability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80345bd3",
   "metadata": {},
   "source": [
    "##### Train variable distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacfbfcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variable_distributions(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d06059c",
   "metadata": {},
   "source": [
    "##### Key Takeaways\n",
    "1. Majority of homes have 2-4 bathrooms.\n",
    "2. Majority of homes are 2-4 bedrooms.\n",
    "3. Majoprity of homes are < 2,500 sqft.\n",
    "4. Homes are in three different counties with the majority of homes in 6040.\n",
    "5. Majority of homes are < 75 years old."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4524b42b",
   "metadata": {},
   "source": [
    "##### Log error distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d44c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['bathrooms', 'bedrooms', 'sqft', 'latitude', \n",
    "            'longitude', 'lot_size', 'tax_value', 'age', \n",
    "            'tax_rate', 'price_per_sqft', 'county_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f33a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_against_target(train, target = 'logerror', var_list = variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e8643",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"county\", y=\"logerror\", data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0411bdc5",
   "metadata": {},
   "source": [
    "##### Key Takeaways\n",
    "1. Higher error rate with homes <= 5 bathrooms.\n",
    "2. Higher error rate with homes between 2-5 bedrooms.\n",
    "3. Homes with a lower sqft have a higher error rate.\n",
    "4. Homes with a lower lot size have a higher error rate.\n",
    "5. Homes with a lower tax value have a higher error rate.\n",
    "6. In this view age has a sligh error rate.\n",
    "7. Homes with a lower tax rate have a higher error rate.\n",
    "8. A lower price per sqft has a higher error rate.\n",
    "9. Homes in Orange county have a higher error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b1b855",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "- I used my clustering as a preprocesing and exloratory step to help develop my hypothesis based on the information above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997d4a7",
   "metadata": {},
   "source": [
    "#### Scale\n",
    "- Here I scaled my data for clustering and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ea7335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty copies to retain the original splits\n",
    "train_scaled = train.copy()\n",
    "validate_scaled = validate.copy()\n",
    "test_scaled = test.copy()\n",
    "#scale\n",
    "scaler = MinMaxScaler()\n",
    "#drop object column\n",
    "cols = train.drop(columns=[\"county\"]).columns.tolist()\n",
    "#fit scaled data\n",
    "train_scaled[cols] = scaler.fit_transform(train[cols])\n",
    "validate_scaled[cols] = scaler.fit_transform(validate[cols])\n",
    "test_scaled[cols] = scaler.fit_transform(test[cols])\n",
    "#add object column back to the split dataframes\n",
    "train_scaled[\"county\"] = train.county.copy()\n",
    "validate_scaled[\"county\"] = validate.county.copy()\n",
    "test_scaled[\"county\"] = test.county.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2dddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create heatmap with scaled data\n",
    "plt.figure(figsize=(8,12))\n",
    "value_heatmap = sns.heatmap(train.corr()[['abs_logerror']].sort_values(by='abs_logerror', ascending=True), \n",
    "                            cmap='coolwarm', vmin=-.5, vmax=.5, annot=True)\n",
    "value_heatmap.set_title('Feautures Correlating with Absolute Logerror')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b8587",
   "metadata": {},
   "source": [
    "##### Key Takeaways\n",
    "1. The above heat map helped me confirm a few takeaways from the exploratory phase.\n",
    "2. I will test age of home, its value, sqft, price per sqft and tax rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c8b343",
   "metadata": {},
   "source": [
    "#### Cluster 1: Latitude and longitude clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78892dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_scaled[['latitude', 'longitude']]\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "train_scaled['cluster'] = kmeans.predict(X)\n",
    "centroids = pd.DataFrame(kmeans.cluster_centers_, columns=X.columns)\n",
    "train_scaled.groupby('cluster')['latitude', 'longitude'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4297056d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 9))\n",
    "\n",
    "for cluster, subset in train_scaled.groupby('cluster'):\n",
    "    plt.scatter(subset.longitude, subset.latitude, label='cluster ' + str(cluster), alpha=.6)\n",
    "centroids.plot.scatter(y='latitude', x='longitude', c='black', marker='x', s=1000, ax=plt.gca(), label='centroid')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Visualizing Cluster Centers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28278418",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08827994",
   "metadata": {},
   "source": [
    "##### Key Takeaways\n",
    "1. Using the elbow method I think gowing with 3 or 4 clusters would be best. I choose to use three as it was a better representation of the counties. \n",
    "2. I would like to further incestigate this to get a better understanding of possible cities with a higher error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d95bb",
   "metadata": {},
   "source": [
    "#### Cluster 2: Log error to age of the home clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5a37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_scaled[['logerror', 'age']]\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "train_scaled['cluster'] = kmeans.predict(X)\n",
    "centroids = pd.DataFrame(kmeans.cluster_centers_, columns=X.columns)\n",
    "train_scaled.groupby('cluster')['logerror', 'age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124dd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 9))\n",
    "\n",
    "for cluster, subset in train_scaled.groupby('cluster'):\n",
    "    plt.scatter(subset.age, subset.logerror, label='cluster ' + str(cluster), alpha=.6)\n",
    "centroids.plot.scatter(y='logerror', x='age', c='black', marker='x', s=1000, ax=plt.gca(), label='centroid')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('logerror')\n",
    "plt.ylabel('age')\n",
    "plt.title('Visualizing Cluster Centers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4056935",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ac3bf2",
   "metadata": {},
   "source": [
    "##### Key Takeaways\n",
    "1. Higer aged home tend to have a higher error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5db6a3",
   "metadata": {},
   "source": [
    "#### Cluster 6: Log error to home square footage clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94897e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_scaled[['logerror', 'sqft']]\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(X)\n",
    "train_scaled['cluster'] = kmeans.predict(X)\n",
    "centroids = pd.DataFrame(kmeans.cluster_centers_, columns=X.columns)\n",
    "train_scaled.groupby('cluster')['logerror', 'sqft'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2299d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 9))\n",
    "\n",
    "for cluster, subset in train_scaled.groupby('cluster'):\n",
    "    plt.scatter(subset.sqft, subset.logerror, label='cluster ' + str(cluster), alpha=.6)\n",
    "centroids.plot.scatter(y='logerror', x='sqft', c='black', marker='x', s=1000, ax=plt.gca(), label='centroid')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('sqft')\n",
    "plt.ylabel('logerror')\n",
    "plt.title('Visualizing Cluster Centers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34222e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec28af12",
   "metadata": {},
   "source": [
    "##### Key Takeaways\n",
    "1. Even though the elbow method may suggest using a n_cluster of three when I choose to run 4 cluster I became curious to that fourth cluster. Which then led my to include in my hypothesis for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86249e25",
   "metadata": {},
   "source": [
    "#### Cluster 7: Log error to price per square footage clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae2394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_scaled[['logerror', 'price_per_sqft']]\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(X)\n",
    "train_scaled['cluster'] = kmeans.predict(X)\n",
    "centroids = pd.DataFrame(kmeans.cluster_centers_, columns=X.columns)\n",
    "train_scaled.groupby('cluster')['logerror', 'price_per_sqft'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d2e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 9))\n",
    "\n",
    "for cluster, subset in train_scaled.groupby('cluster'):\n",
    "    plt.scatter(subset.price_per_sqft, subset.logerror, label='cluster ' + str(cluster), alpha=.6)\n",
    "\n",
    "centroids.plot.scatter(y='logerror', x='price_per_sqft', c='black', marker='x', s=1000, ax=plt.gca(), label='centroid')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('price_per_sqft')\n",
    "plt.ylabel('loerror')\n",
    "plt.title('Visualizing Cluster Centers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dc452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988a3252",
   "metadata": {},
   "source": [
    "##### Key Takeaways\n",
    "1. Again here when I ran 5 clusters I saw the fifth clusters to which I have also include in my hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e01fc",
   "metadata": {},
   "source": [
    "#### Cluster 8: Log error to lot size clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55098d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_scaled[['logerror', 'lot_size']]\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "train_scaled['cluster'] = kmeans.predict(X)\n",
    "centroids = pd.DataFrame(kmeans.cluster_centers_, columns=X.columns)\n",
    "train_scaled.groupby('cluster')['logerror', 'lot_size'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dce6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 9))\n",
    "\n",
    "for cluster, subset in train_scaled.groupby('cluster'):\n",
    "    plt.scatter(subset.lot_size, subset.logerror, label='cluster ' + str(cluster), alpha=.6)\n",
    "\n",
    "centroids.plot.scatter(y='logerror', x='lot_size', c='black', marker='x', s=1000, ax=plt.gca(), label='centroid')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('lot_size')\n",
    "plt.ylabel('loerror')\n",
    "plt.title('Visualizing Cluster Centers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f80e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a965e676",
   "metadata": {},
   "source": [
    "##### Key Takeaway\n",
    "1. I again decided to test this 4th cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe75689",
   "metadata": {},
   "source": [
    "#### Cluster 9: Log error to home value clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfff262",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_scaled[['logerror', 'tax_value']]\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "train_scaled['cluster'] = kmeans.predict(X)\n",
    "centroids = pd.DataFrame(kmeans.cluster_centers_, columns=X.columns)\n",
    "train_scaled.groupby('cluster')['logerror', 'tax_value'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3123d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 9))\n",
    "\n",
    "for cluster, subset in train_scaled.groupby('cluster'):\n",
    "    plt.scatter(subset.tax_value, subset.logerror, label='cluster ' + str(cluster), alpha=.6)\n",
    "\n",
    "centroids.plot.scatter(y='logerror', x='tax_value', c='black', marker='x', s=1000, ax=plt.gca(), label='centroid')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('tax_value')\n",
    "plt.ylabel('loerror')\n",
    "plt.title('Visualizing Cluster Centers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc1a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf6bdef",
   "metadata": {},
   "source": [
    "##### Key Takeaways\n",
    "1. Home value in exploration had a high error rate.\n",
    "2. I decided to test the 4th cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687c1dbf",
   "metadata": {},
   "source": [
    "### Hypothesis derived from the explore phase\n",
    "- Is a higher log error dependent on homes over 50 years old? (Cluster - 2)\n",
    "- Is a higher log error dependent on homes less 1000 sqft? (Cluster - 6)\n",
    "- Is a higher log error dependent on homes who's ppsqft is less 200? (Cluster - 7)\n",
    "- Is a higher log error dependent on homes with a smaller lot size? (Cluster - 8)\n",
    "- Is a higher log error dependent on less expensive homes? (Cluster - 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e010503",
   "metadata": {},
   "source": [
    "### Statistical Testing\n",
    "- I'll be testing the above listed hypothesis using the chi2 method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647a799",
   "metadata": {},
   "source": [
    "##### Is a higher log error dependent on homes over 50 years old? (Cluster - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee17f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "Null = 'Is independent'\n",
    "Alternate = 'Is dependent'\n",
    "\n",
    "observed = pd.crosstab(train.logerror > 0, train.age > 50)\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed)\n",
    "\n",
    "print(f'chi^2 = {chi2:.4f}')\n",
    "print(f'p     = {p:.4f}')\n",
    "\n",
    "print('\\n')\n",
    "if p < alpha:\n",
    "    print(f'We reject the null and accept the alternate: {Alternate}')\n",
    "else:\n",
    "    print(f'We fail to reject the null and accept the null: {Null}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c4f1f6",
   "metadata": {},
   "source": [
    "##### Is a higher log error dependent on homes less 1000 sqft? (Cluster - 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d60dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Null = 'Is independent'\n",
    "Alternate = 'Is dependent'\n",
    "\n",
    "observed = pd.crosstab(train.logerror > 0, train.sqft > 1000)\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed)\n",
    "\n",
    "print(f'chi^2 = {chi2:.4f}')\n",
    "print(f'p     = {p:.4f}')\n",
    "\n",
    "print('\\n')\n",
    "if p < alpha:\n",
    "    print(f'We reject the null and accept the alternate: {Alternate}')\n",
    "else:\n",
    "    print(f'We fail to reject the null and accept the null: {Null}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a75346d",
   "metadata": {},
   "source": [
    "##### Is a higher log error dependent on homes who's ppsqft is less 500? (Cluster - 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb6a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Null = 'Is independent'\n",
    "Alternate = 'Is dependent'\n",
    "\n",
    "observed = pd.crosstab(train.logerror > 0, train.price_per_sqft < 500)\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed)\n",
    "\n",
    "print(f'chi^2 = {chi2:.4f}')\n",
    "print(f'p     = {p:.4f}')\n",
    "\n",
    "print('\\n')\n",
    "if p < alpha:\n",
    "    print(f'We reject the null and accept the alternate: {Alternate}')\n",
    "else:\n",
    "    print(f'We fail to reject the null and accept the null: {Null}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86cbe2",
   "metadata": {},
   "source": [
    "##### Is a higher log error dependent on homes with a smaller lot size? (Cluster - 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d3d32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Null = 'Is independent'\n",
    "Alternate = 'Is dependent'\n",
    "\n",
    "observed = pd.crosstab(train.logerror > 0, train.lot_size < 236)\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed)\n",
    "\n",
    "print(f'chi^2 = {chi2:.4f}')\n",
    "print(f'p     = {p:.4f}')\n",
    "\n",
    "print('\\n')\n",
    "if p < alpha:\n",
    "    print(f'We reject the null and accept the alternate: {Alternate}')\n",
    "else:\n",
    "    print(f'We fail to reject the null and accept the null: {Null}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868660f",
   "metadata": {},
   "source": [
    "##### Is a higher log error dependent on less expensive homes? (Cluster - 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98051f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Null = 'Is independent'\n",
    "Alternate = 'Is dependent'\n",
    "\n",
    "observed = pd.crosstab(train.logerror > 0, train.tax_value < 250000)\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed)\n",
    "\n",
    "print(f'chi^2 = {chi2:.4f}')\n",
    "print(f'p     = {p:.4f}')\n",
    "\n",
    "print('\\n')\n",
    "if p < alpha:\n",
    "    print(f'We reject the null and accept the alternate: {Alternate}')\n",
    "else:\n",
    "    print(f'We fail to reject the null and accept the null: {Null}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6cbe1d",
   "metadata": {},
   "source": [
    "##### Key Takeaways\n",
    "1. Homes older than 50 years old have a higher log error rate.\n",
    "2. Homes less than 1000sqft have a higher log error rate.\n",
    "3. Homes less than 250,000 have a higher log error rate.\n",
    "4. All others failed to reject the null."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c660a0f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194b2da2",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "- Based on exploration and testing I decided to model on the following features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_scaled[['age', 'bathrooms', 'bedrooms', 'sqft', 'price_per_sqft', 'lot_size', 'tax_value']]#features\n",
    "y_train = train.logerror\n",
    "X_validate = validate_scaled[['age', 'bathrooms', 'bedrooms', 'sqft', 'price_per_sqft', 'lot_size', 'tax_value']]#features\n",
    "y_validate = validate.logerror\n",
    "X_test = test_scaled[['age', 'bathrooms', 'bedrooms', 'sqft', 'price_per_sqft', 'lot_size', 'tax_value']]#features\n",
    "y_test = test.logerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda5978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.rfe(X_train,y_train,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a48af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.rfe(X_train,y_train,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a728b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.select_kbest(X_train,y_train,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b3983",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.select_kbest(X_train,y_train,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5f9ff8",
   "metadata": {},
   "source": [
    "##### Key Takeaways\n",
    "1. What is interesting here is age does not factor into any of the tests.\n",
    "2. Sqft is the best feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d66a5",
   "metadata": {},
   "source": [
    "### Regression Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68af2e3a",
   "metadata": {},
   "source": [
    "# Model on Sqft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2aa759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline function calculates baseline and adds columns to the dataframe\n",
    "evaluate.get_baseline(train,train[['sqft']], train['logerror'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501a94a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.get_residuals(train, train['logerror'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b335e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.plot_residual(train, train[['sqft']], train['logerror'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59943b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.regression_errors(train, train['logerror'], train.yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.baseline_mean_errors(train, train['logerror'], train.yhat_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.better_than_baseline(regression_errors = True, baseline_mean_errors = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d30303b",
   "metadata": {},
   "source": [
    "##### Key Takeaways\n",
    "1. Modeling on sqft and log error the model did better then the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bcab25",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcfbc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model_baseline(y_train, y_validate, 'logerror')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b815b699",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b1cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear_regression(y_train, X_train, y_validate, X_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170f4a4c",
   "metadata": {},
   "source": [
    "#### LassoLars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb3d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lassolars(y_train, X_train, y_validate, X_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2188fa",
   "metadata": {},
   "source": [
    "#### Tpolynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec218e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.polynomialregression(y_train, X_train, y_validate, X_validate, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0f6db0",
   "metadata": {},
   "source": [
    "##### Key Takeaways\n",
    "1. Train RMSE on the Liner Regression model preformed better then the other two models.\n",
    "2. With an RMSE of 0.1657 I chose to test on the liners regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df3ddf",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78843e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear_regression_test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b595fa",
   "metadata": {},
   "source": [
    "##### Key Takeaways\n",
    "- Test model did not perform as well as I hope going over by .100th of a point with an RMSE of 0.175 using sqft and logerror."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34287876",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597dc7e7",
   "metadata": {},
   "source": [
    "- Used the 2017 properties dataset to derive features that are driving log errors in Zestimates.\n",
    "- The best feature modeled is based on homes less than 1000 sqft.\n",
    "- Although, home value was not featured in my rfe test, price_per_sqft was the second best feature.\n",
    "- I modeled the best performing feature and tested the model with an RMSE of 0.175."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d47a3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
